{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8a83be6",
   "metadata": {},
   "source": [
    "# MOWO - Mouse Only Walk Ooooh\n",
    "\n",
    "This notebook reads in the processed training data to fine-tune YOLO model (https://github.com/qqwweee/keras-yolo3/tree/master/model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2626e74",
   "metadata": {
    "_cell_guid": "01a137bc-ec6b-487a-9c1b-161ff4e2830c",
    "_uuid": "f6d4dd2e-adb1-4ad2-9b78-866bfe3c256c",
    "execution": {
     "iopub.execute_input": "2022-01-29T20:41:36.925655Z",
     "iopub.status.busy": "2022-01-29T20:41:36.923875Z",
     "iopub.status.idle": "2022-01-29T22:13:36.496356Z",
     "shell.execute_reply": "2022-01-29T22:13:36.496798Z",
     "shell.execute_reply.started": "2022-01-29T19:09:26.452854Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 5519.583591,
     "end_time": "2022-01-29T22:13:36.497088",
     "exception": false,
     "start_time": "2022-01-29T20:41:36.913497",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "from keras.layers import Input, Lambda\n",
    "from keras.models import load_model\n",
    "from keras.models import Model\n",
    "from keras.optimizers import adam_v2\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "import keras\n",
    "import pickle\n",
    "import gc\n",
    "import sys\n",
    "\n",
    "sys.path.append('../scripts/')\n",
    "import inference_utils\n",
    "from yolo import *\n",
    "\n",
    "from model import preprocess_true_boxes, yolo_body, tiny_yolo_body, yolo_loss\n",
    "from utils import get_random_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1b3ac8",
   "metadata": {},
   "source": [
    "# Training Functions  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8e84ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classes(classes_path):\n",
    "    '''loads the classes'''\n",
    "    with open(classes_path) as f:\n",
    "        class_names = f.readlines()\n",
    "    class_names = [c.strip() for c in class_names]\n",
    "    return class_names\n",
    "\n",
    "def get_anchors(anchors_path):\n",
    "    '''loads the anchors from a file'''\n",
    "    with open(anchors_path) as f:\n",
    "        anchors = f.readline()\n",
    "    anchors = [float(x) for x in anchors.split(',')]\n",
    "    return np.array(anchors).reshape(-1, 2)\n",
    "\n",
    "\n",
    "def create_model(input_shape, anchors, num_classes, load_pretrained=True, freeze_body=2,\n",
    "            weights_path='../input/model-data/model_data/yolo_weights.h5'):\n",
    "    '''create the training model'''\n",
    "    K.clear_session() # get a new session\n",
    "    image_input = Input(shape=(None, None, 3))\n",
    "    h, w = input_shape\n",
    "    num_anchors = len(anchors)\n",
    "\n",
    "    y_true = [Input(shape=(h//{0:32, 1:16, 2:8}[l], w//{0:32, 1:16, 2:8}[l], \\\n",
    "        num_anchors//3, num_classes+5)) for l in range(3)]\n",
    "\n",
    "    model_body = yolo_body(image_input, num_anchors//3, num_classes)\n",
    "    print('Create YOLOv3 model with {} anchors and {} classes.'.format(num_anchors, num_classes))\n",
    "\n",
    "    if load_pretrained:\n",
    "        model_body.load_weights(weights_path, by_name=True, skip_mismatch=True)\n",
    "        print('Load weights {}.'.format(weights_path))\n",
    "        if freeze_body in [1, 2]:\n",
    "            # Freeze darknet53 body or freeze all but 3 output layers.\n",
    "            num = (185, len(model_body.layers)-3)[freeze_body-1]\n",
    "            for i in range(num): model_body.layers[i].trainable = False\n",
    "            print('Freeze the first {} layers of total {} layers.'.format(num, len(model_body.layers)))\n",
    "\n",
    "    # get output of second last layers and create bottleneck model of it\n",
    "    out1=model_body.layers[246].output\n",
    "    out2=model_body.layers[247].output\n",
    "    out3=model_body.layers[248].output\n",
    "    bottleneck_model = Model([model_body.input, *y_true], [out1, out2, out3])\n",
    "\n",
    "    # create last layer model of last layers from yolo model\n",
    "    in0 = Input(shape=bottleneck_model.output[0].shape[1:].as_list())\n",
    "    in1 = Input(shape=bottleneck_model.output[1].shape[1:].as_list())\n",
    "    in2 = Input(shape=bottleneck_model.output[2].shape[1:].as_list())\n",
    "    last_out0=model_body.layers[249](in0)\n",
    "    last_out1=model_body.layers[250](in1)\n",
    "    last_out2=model_body.layers[251](in2)\n",
    "    model_last=Model(inputs=[in0, in1, in2], outputs=[last_out0, last_out1, last_out2])\n",
    "    print(anchors)\n",
    "    print(num_classes)\n",
    "    model_loss_last =Lambda(yolo_loss, output_shape=(1,), name='yolo_loss',\n",
    "        arguments={'anchors': anchors, 'num_classes': num_classes, 'ignore_thresh': 0.5})(\n",
    "        [*model_last.output, *y_true])\n",
    "    last_layer_model = Model([in0,in1,in2, *y_true], model_loss_last)\n",
    "\n",
    "\n",
    "    model_loss = Lambda(yolo_loss, output_shape=(1,), name='yolo_loss',\n",
    "        arguments={'anchors': anchors, 'num_classes': num_classes, 'ignore_thresh': 0.5})(\n",
    "        [*model_body.output, *y_true])\n",
    "    model = Model([model_body.input, *y_true], model_loss)\n",
    "\n",
    "    return model, bottleneck_model, last_layer_model\n",
    "\n",
    "def data_generator(annotation_lines, batch_size, input_shape, anchors, num_classes, random=False, verbose=False):\n",
    "    '''data generator for fit_generator'''\n",
    "    n = len(annotation_lines)\n",
    "    i = 0\n",
    "    while True:\n",
    "        image_data = []\n",
    "        box_data = []\n",
    "        for b in range(batch_size):\n",
    "            if i==0 and random:\n",
    "                np.random.shuffle(annotation_lines)\n",
    "            image, box = get_random_data(annotation_lines[i], input_shape, random=random)\n",
    "            image_data.append(image)\n",
    "            box_data.append(box)\n",
    "            i = (i+1) % n\n",
    "        image_data = np.array(image_data)\n",
    "        if verbose:\n",
    "            print(\"Progress: \",i,\"/\",n)\n",
    "        box_data = np.array(box_data)\n",
    "        y_true = preprocess_true_boxes(box_data, input_shape, anchors, num_classes)\n",
    "        yield [image_data, *y_true], np.zeros(batch_size)\n",
    "\n",
    "def data_generator_wrapper(annotation_lines, batch_size, input_shape, anchors, num_classes, random=False, verbose=False):\n",
    "    n = len(annotation_lines)\n",
    "    if n==0 or batch_size<=0: return None\n",
    "    return data_generator(annotation_lines, batch_size, input_shape, anchors, num_classes, random, verbose)\n",
    "\n",
    "def bottleneck_generator(annotation_lines, batch_size, input_shape, anchors, num_classes, bottlenecks,random_=False):\n",
    "    n = len(annotation_lines)\n",
    "    print(n)\n",
    "    i = 0\n",
    "    while True:\n",
    "        box_data = []\n",
    "        b0=np.zeros((batch_size,bottlenecks[0].shape[1],bottlenecks[0].shape[2],bottlenecks[0].shape[3]))\n",
    "        b1=np.zeros((batch_size,bottlenecks[1].shape[1],bottlenecks[1].shape[2],bottlenecks[1].shape[3]))\n",
    "        b2=np.zeros((batch_size,bottlenecks[2].shape[1],bottlenecks[2].shape[2],bottlenecks[2].shape[3]))\n",
    "        for b in range(batch_size):\n",
    "            _, box = get_random_data(annotation_lines[i], input_shape, random=random_)\n",
    "            box_data.append(box)\n",
    "            b0[b]=bottlenecks[0][i]\n",
    "            b1[b]=bottlenecks[1][i]\n",
    "            b2[b]=bottlenecks[2][i]\n",
    "            i = (i+1) % n\n",
    "        box_data = np.array(box_data)\n",
    "        y_true = preprocess_true_boxes(box_data, input_shape, anchors, num_classes)\n",
    "        yield [b0, b1, b2, *y_true], np.zeros(batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad40d89b",
   "metadata": {
    "papermill": {
     "duration": 3.03313,
     "end_time": "2022-01-29T22:15:03.825081",
     "exception": false,
     "start_time": "2022-01-29T22:15:00.791951",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def _main():\n",
    "    \n",
    "    ext = 'mouse'\n",
    "    annotation_path = '../input/mouse-walking-data/train.txt' # if this changes, the bottlenecks will need to be updated first\n",
    "    log_dir      = './logs/'\n",
    "    bott_dir = '../input/bottlenecks/logs/'\n",
    "    save_path = './'\n",
    "    weight_path = '../input/trained-weights/'\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    classes_path = '../input/model-data/model_data/mouse_classes.txt'\n",
    "    anchors_path = '../input/model-data/model_data/yolo_anchors_mouse.txt'\n",
    "    class_names  = get_classes(classes_path)\n",
    "    num_classes  = len(class_names)\n",
    "    anchors = get_anchors(anchors_path)\n",
    "\n",
    "    training_stage = 1 # you need to start from 0 if train.txt is changed\n",
    "    # specify where the training should start (0 is from bottlenecks, 1 is from a draft model)\n",
    "    \n",
    "    input_shape = (64,256) # reshaping the image, multiple of 32, must match existing bottlenecks if it's being used\n",
    "\n",
    "    model, bottleneck_model, last_layer_model = create_model(input_shape, anchors, num_classes,\n",
    "            freeze_body=2, weights_path='../input/model-data/model_data/yolo.h5') # make sure you know what you freeze\n",
    "\n",
    "    checkpoint = ModelCheckpoint(log_dir , monitor='val_loss', save_weights_only=True, save_best_only=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1)\n",
    "\n",
    "    val_split = 0.1\n",
    "    \n",
    "    with open(annotation_path) as f:\n",
    "        all_lines = f.readlines()\n",
    "    \n",
    "    num_val = int(len(all_lines)*val_split)\n",
    "    num_train = len(all_lines) - num_val\n",
    "\n",
    "    # Train with frozen layers first, to get a stable loss.\n",
    "    # Adjust num epochs to your dataset. This step is enough to obtain a not bad model.\n",
    "    # perform bottleneck training\n",
    "    if training_stage == -1 :\n",
    "        print(\"calculating bottlenecks\")\n",
    "        b = [0 , 1024]\n",
    "        batch_size=32\n",
    "        count = 0\n",
    "        while b[1] < len(all_lines):\n",
    "            print(count)\n",
    "            lines = all_lines[b[0]:b[1]]\n",
    "            bottlenecks = bottleneck_model.predict(data_generator_wrapper(lines, batch_size, input_shape, anchors, num_classes, random=True, verbose=True),steps=(len(lines)//batch_size)+1, max_queue_size=1)\n",
    "            pickle.dump(bottlenecks[0], open(log_dir + \"bottlenecks_\"  + str(count) + ext + \"0.pickle\",'wb'))\n",
    "            pickle.dump(bottlenecks[1], open(log_dir + \"bottlenecks_\"  + str(count) + ext + \"1.pickle\",'wb'))\n",
    "            pickle.dump(bottlenecks[2], open(log_dir + \"bottlenecks_\"  + str(count) + ext + \"2.pickle\",'wb'))\n",
    "            b[0] += 1024\n",
    "            b[1] += 1024\n",
    "            count += 1\n",
    "\n",
    "    batch_size=32\n",
    "    \n",
    "    if training_stage == -1 :\n",
    "        bott_dir = log_dir\n",
    "\n",
    "        \n",
    "    if training_stage <= 0 :\n",
    "        b = [0 , 1024]\n",
    "        last_layer_model.compile(optimizer=adam_v2.Adam(0.00005), loss={'yolo_loss': lambda y_true, y_pred: y_pred})\n",
    "        count = 0\n",
    "        while b[1] < len(all_lines):\n",
    "            print(count)\n",
    "            lines = all_lines[b[0]:b[1]]\n",
    "            \n",
    "            b[0] += 1024\n",
    "            b[1] += 1024\n",
    "            \n",
    "            num_val = int(len(lines)*val_split)\n",
    "            num_train = len(lines) - num_val\n",
    "            \n",
    "            dict_bot0 = pickle.load(open(bott_dir + \"bottlenecks_\" + str(count)+ ext + \"0.pickle\",'rb'))\n",
    "            dict_bot1 = pickle.load(open(bott_dir + \"bottlenecks_\" + str(count)+ ext + \"1.pickle\",'rb'))\n",
    "            dict_bot2 = pickle.load(open(bott_dir + \"bottlenecks_\" + str(count)+ ext + \"2.pickle\",'rb'))\n",
    "            dict_bot = {\"bot0\":dict_bot0,\"bot1\":dict_bot1,\"bot2\":dict_bot2}\n",
    "            \n",
    "            bottlenecks_train=[dict_bot[\"bot0\"][:num_train], dict_bot[\"bot1\"][:num_train], dict_bot[\"bot2\"][:num_train]]\n",
    "\n",
    "            bottlenecks_val=[dict_bot[\"bot0\"][num_train:], dict_bot[\"bot1\"][num_train:], dict_bot[\"bot2\"][num_train:]]\n",
    "            \n",
    "            print(\"Training last layers with bottleneck features : \" + str(count))\n",
    "            print('with {} samples, val on {} samples and batch size {}.'.format(num_train, num_val, batch_size) , end = '\\r')\n",
    "            \n",
    "            \n",
    "            last_layer_model.fit(bottleneck_generator(lines[:num_train], batch_size, input_shape, anchors, num_classes, bottlenecks_train, random_=True),\n",
    "                    steps_per_epoch=max(1, num_train//batch_size),\n",
    "                    validation_data=bottleneck_generator(lines[num_train:], batch_size, input_shape, anchors, num_classes, bottlenecks_val, random_=True),\n",
    "                    validation_steps=max(1, num_val//batch_size),epochs=20,initial_epoch=0, max_queue_size=1)\n",
    "            \n",
    "            del(dict_bot)\n",
    "            del(dict_bot0)\n",
    "            del(dict_bot1)\n",
    "            del(dict_bot2)\n",
    "            del(bottlenecks_val)\n",
    "            del(bottlenecks_train)\n",
    "            del(lines)\n",
    "            \n",
    "            gc.collect()\n",
    "            \n",
    "            count += 1\n",
    "            \n",
    "            if count > 16:\n",
    "                break\n",
    "            \n",
    "        model.save_weights(log_dir + 'trained_weights_stage_0_'  + ext + '.h5')\n",
    "\n",
    "    # Unfreeze and continue training, to fine-tune.\n",
    "    # Train longer if the result is not good.\n",
    "    if training_stage == 0 :\n",
    "        weight_path_ = log_dir + 'trained_weights_stage_0_'  + ext + '.h5'\n",
    "        print(\"loading from log, Stage 1 \" + weight_path_)\n",
    "        model.load_weights(weight_path_)\n",
    "        \n",
    "    else:\n",
    "        print(model.summary())\n",
    "        weight_path_ = weight_path + 'trained_weights_stage_0_'  + ext + '.h5'\n",
    "        print(\"Starting at Stage 1, loading \" + weight_path_)\n",
    "        model.load_weights(weight_path_)\n",
    "\n",
    "        \n",
    "    print('Unfreeze all of the layers.')\n",
    "    for i in range(len(model.layers)):\n",
    "        model.layers[i].trainable = True\n",
    "        \n",
    "    model.compile(optimizer= adam_v2.Adam(0.0001), loss={'yolo_loss': lambda y_true, y_pred: y_pred}) # recompile to apply the change\n",
    "\n",
    "    batch_size = 64 # note that more GPU memory is required after unfreezing the body\n",
    "    print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size))\n",
    "\n",
    "    model.fit(data_generator_wrapper(all_lines[:num_train], batch_size, input_shape, anchors, num_classes),\n",
    "            steps_per_epoch=max(1, num_train//batch_size),\n",
    "            validation_data=data_generator_wrapper(all_lines[num_train:], batch_size, input_shape, anchors, num_classes),\n",
    "            validation_steps=max(1, num_val//batch_size),\n",
    "            epochs=30,\n",
    "            initial_epoch=0,\n",
    "            callbacks=[checkpoint, reduce_lr, early_stopping])\n",
    "\n",
    "    model.save_weights(log_dir + 'trained_weights_final_'  + ext + '.h5')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46202860",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    _main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee23f008",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5621.193263,
   "end_time": "2022-01-29T22:15:09.388341",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-01-29T20:41:28.195078",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
